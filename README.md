# Advanced RAG Chatbot with Caching & Multi-Format Support

This project provides a powerful, open-source Retrieval-Augmented Generation (RAG) chatbot built with Python. It intelligently processes multiple document types (`.pdf`, `.docx`, `.txt`), uses an efficient caching system to ensure fast startups, and automatically detects file changes to keep its knowledge base perfectly up-to-date.

The entire system is designed for easy use on a local machine and is optimized for deployment on High-Performance Computing (HPC) clusters using Singularity.

##  Features

* ** Multi-Document Support:** Ingests and understands various file formats, including **PDF**, **Microsoft Word** (`.doc`, `.docx`), and **plain text** (`.txt`).
* ** Intelligent Caching:** Automatically saves a processed text version of complex files. On subsequent runs, it loads from this fast cache instead of reprocessing the original document, saving significant time.
* ** Automatic Change Detection:** Uses **MD5 hashing** to detect if a source file has been modified. The chatbot only spends time reprocessing files that are new or have been changed, making knowledge base updates incredibly efficient.
* ** HPC Ready:** Includes detailed, best-practice instructions for deploying the Ollama server in a containerized environment on HPC clusters using **Singularity**.
* ** Fully Open-Source:** Built with leading open-source libraries like **LangChain**, **Ollama**, **FAISS**, and **Hugging Face** models.

---

## ‚öôÔ∏è How It Works

The script employs a smart processing pipeline to minimize redundant work and maximize speed.

1.  **Scan & Hash:** The script scans the `data/` directory and calculates a unique hash (MD5 checksum) for each file.
2.  **Compare & Decide:** It compares these hashes to a record of previously processed files.
    * If a file is **new** or its **hash has changed**, it's marked for full processing.
    * If a file's hash is **unchanged**, the script will load its content directly from the fast text cache in the `processed_texts/` folder.
3.  **Process & Cache:** New or modified PDF and DOCX files are read, their text is extracted, and this plain text is saved to the cache for future use.
4.  **Build Vector Store:** The text content from all documents is split into chunks, converted into embeddings, and stored in a **FAISS vector database**. This database is what the chatbot uses to find relevant information to answer questions.



This workflow ensures that you only pay the "cost" of processing a complex document once.

---

## üìÇ Project Structure

```
rag-chatbot/
‚îú‚îÄ‚îÄ data/                  # <-- Place your source documents here
‚îÇ   ‚îî‚îÄ‚îÄ your_document.pdf
‚îú‚îÄ‚îÄ processed_texts/       # <-- Cached plain text versions of documents
‚îÇ   ‚îî‚îÄ‚îÄ your_document_processed.txt
‚îú‚îÄ‚îÄ vectorstore/           # <-- The FAISS vector store is saved here
‚îú‚îÄ‚îÄ .gitignore             # <-- Specifies files/folders for git to ignore
‚îú‚îÄ‚îÄ app.py                 # <-- The main Python application script
‚îú‚îÄ‚îÄ README.md              # <-- This file
‚îî‚îÄ‚îÄ requirements.txt       # <-- A list of Python dependencies
```

---

## üöÄ Local Setup and Installation (Conda)

This project uses **Conda** to manage its environment.

#### 1. Prerequisites
* [Git](https://git-scm.com/downloads) installed.
* [Conda](https://www.anaconda.com/download) (Anaconda or Miniconda) installed.
* [Ollama](https://ollama.com/) installed and running.

#### 2. Clone the Repository
```bash
git clone [https://github.com/Erzengel583/rag-chatbot](https://github.com/Erzengel583/rag-chatbot)
cd rag-chatbot
```

#### 3. Create and Activate the Conda Environment
```bash
# Create the environment with Python 3.10
conda create --name rag-chatbot-env python=3.10 -y

# Activate the environment
conda activate rag-chatbot-env
```

#### 4. Install Dependencies
Install all required packages from the `requirements.txt` file.
```bash
pip install -r requirements.txt
```

#### 5. Download an Ollama LLM
Ensure the Ollama application is running, then open a terminal and pull a model.
```bash
ollama pull llama3
```

---

## üõ∞Ô∏è HPC Deployment with Singularity

For HPC environments, running Ollama inside a **Singularity container** is the recommended method for portability and stability.

#### 1. Create the Singularity Definition File
On your local computer, create a file named `ollama.def`. This file defines how to build the container.
```sif
Bootstrap: docker
From: ollama/ollama:latest

%description
    This container runs the Ollama server and client.

%runscript
    # This executes when you use 'singularity run'.
    # It passes all command-line arguments ($@) to the ollama command.
    echo "Executing: ollama $@"
    exec ollama "$@"

%startscript
    # This runs when you use 'singularity instance start'.
    echo "Starting Ollama server..."
    ollama serve
```

#### 2. Build and Transfer the Image
Build the image on a machine where you have `sudo` permissions (like your local Linux computer) and copy it to the HPC.
```bash
# Build the image locally
sudo singularity build ollama.sif ollama.def

# Transfer the image to your HPC home directory
scp ollama.sif your_username@hpc.cluster.edu:~/
```

#### 3. Run Ollama on a Compute Node
After starting an interactive job on a GPU node, choose one of the following methods to run the Ollama server.

**Method A: Using `singularity instance` (Recommended for Services)**
This method runs Ollama as a managed background service.

```bash
# Start the service instance with GPU access (--nv)
singularity instance start --nv ollama.sif ollama-service

# Pull your model by executing a command inside the instance
singularity exec instance://ollama-service ollama pull llama3

# When finished, stop the service cleanly
singularity instance stop ollama-service
```

**Method B: Using `singularity run` (Direct Execution)**
This method uses standard Linux commands to run the server in the background.

```bash
# Run the 'serve' command and send it to the background (&)
singularity run --nv ollama.sif serve &

# Pull your model by passing arguments directly
singularity run ollama.sif pull llama3

# When finished, stop the server process
pkill -f "singularity run --nv ollama.sif serve"
```

---

## üí¨ How to Use the Chatbot

The application is designed to be simple to run after setup.

1.  **Add Documents:** Place your `.pdf`, `.docx`, `.doc`, and `.txt` files into the `data/` folder.
2.  **Run the Application:** Open your terminal, activate the Conda environment, and run the script.
    ```bash
    conda activate rag-chatbot-env
    python app.py
    ```
3.  **Follow Prompts:**
    * On the **first run**, the script will automatically process all documents and build the vector store.
    * On **subsequent runs**, it will ask if you want to check for new or modified files. Type `y` and press Enter to update the knowledge base. If you type `n`, it will start instantly using the existing database.
4.  **Start Chatting:** Once you see the "CHATBOT READY!" message, you can start asking questions based on your documents! Type `exit` to end the session.